{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 25\n",
    "STRIDE = 20\n",
    "LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download EN wikipedia locally.\n",
    "\n",
    "# import requests\n",
    "# import io\n",
    "\n",
    "# WIKIPEDIA = [\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/0.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/1.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/2.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/3.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/4.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/5.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/6.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/7.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/8.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/9.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/10.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/11.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/12.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/13.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/14.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/15.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/16.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/17.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/18.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/19.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/20.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/21.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/22.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/23.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/24.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/25.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/26.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/27.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/28.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/29.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/30.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/31.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/32.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/33.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/34.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/35.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/36.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/37.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/38.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/39.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/40.parquet\"]\n",
    "\n",
    "# for i, url in enumerate(WIKIPEDIA):\n",
    "#     print(url)\n",
    "#     pd.read_parquet(io.BytesIO(requests.get(url).content)).rename(columns={'id': 'article_id'}).to_parquet(f\"wiki/wiki_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE_ID_BLACKLIST = [\n",
    "    3613433,\n",
    "    287543,\n",
    "    4489,\n",
    "    15039439,\n",
    "    236331,\n",
    "    13262036,\n",
    "    3634424,\n",
    "    155117,\n",
    "    43139543,\n",
    "    7397019,\n",
    "    72819,\n",
    "    47702,\n",
    "    255468,\n",
    "    8526396,\n",
    "    10693894,\n",
    "    5710507,\n",
    "    387814,\n",
    "    18834674,\n",
    "    1618662,\n",
    "    18721790,\n",
    "    19385671,\n",
    "    16637675,\n",
    "    89775,\n",
    "    231705,\n",
    "    344902,\n",
    "    67193,\n",
    "    3938382,\n",
    "    16831059,\n",
    "    8082932,\n",
    "    218407,\n",
    "    15179951,\n",
    "    25211885,\n",
    "    27988331,\n",
    "    5374,\n",
    "    2333250,\n",
    "    35334391,\n",
    "    1081926,\n",
    "    1784650,\n",
    "    20780702,\n",
    "    97645,\n",
    "    285245,\n",
    "    2101268,\n",
    "    7980471,\n",
    "    18855505,\n",
    "    411888,\n",
    "    150389,\n",
    "    2500,\n",
    "    236648,\n",
    "    19666880,\n",
    "    19629283,\n",
    "    8387128,\n",
    "    173634,\n",
    "    568911,\n",
    "    2324040,\n",
    "    374422,\n",
    "    156115,\n",
    "    21323216,\n",
    "    146573,\n",
    "    1371871,\n",
    "    46963,\n",
    "    2957039,\n",
    "    20611030,\n",
    "    2480306,\n",
    "    18842168,\n",
    "    49084,\n",
    "    12137890,\n",
    "    17322701,\n",
    "    11415141,\n",
    "    188018,\n",
    "    2460,\n",
    "    5575722,\n",
    "    106457,\n",
    "    292437,\n",
    "    154823,\n",
    "    211430,\n",
    "    220255,\n",
    "    2553981,\n",
    "    1787105,\n",
    "    10605960,\n",
    "    3952114,\n",
    "    19019270,\n",
    "    20603860, \n",
    "    21304415, \n",
    "    6679136, \n",
    "    49605932, \n",
    "    158934\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117380"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_top_counts(min_count):\n",
    "    selected = {}\n",
    "    with open('data/links.txt', 'r', encoding='utf-8', errors='replace') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Filter out hashtag pages\n",
    "                if line[0] == '#':\n",
    "                    continue\n",
    "\n",
    "                # Filter out category pages\n",
    "                skip = False\n",
    "                for substring in ['Category:', 'File:', 'Wikipedia:', 'user:']:\n",
    "                    if substring in line:\n",
    "                        skip = True\n",
    "                if skip:\n",
    "                    continue\n",
    "                    \n",
    "                # Filter out aliases and sublinks\n",
    "                for substring in ['#', '|']:\n",
    "                    if substring in line:\n",
    "                        line = line.split(substring, 1)[0]\n",
    "\n",
    "                line = line.strip()\n",
    "\n",
    "                if line in selected:\n",
    "                    selected[line] += 1\n",
    "                else:\n",
    "                    selected[line] = 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    selected = pd.DataFrame(\n",
    "        list(\n",
    "            filter(\n",
    "                lambda x: x['count'] >= min_count,\n",
    "                sorted(\n",
    "                    [\n",
    "                        {\n",
    "                            'lower_title': x.lower().strip(), \n",
    "                            'count': y\n",
    "                        } \n",
    "                        for x, y in selected.items()\n",
    "                    ],\n",
    "                    key=lambda x: x['count'],\n",
    "                    reverse=True\n",
    "                )   \n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return selected\n",
    "\n",
    "selected = select_top_counts(MIN_COUNT)\n",
    "selected.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Set \n",
    "\n",
    "def filter_df(df, selected):\n",
    "    df = df.rename(columns={'id': 'article_id'})\n",
    "    df['lower_title'] = df.title.apply(lambda x: x.strip().lower())\n",
    "    return df.merge(selected, on='lower_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_0.parquet\n",
      "wiki_1.parquet\n",
      "wiki_10.parquet\n",
      "wiki_11.parquet\n",
      "wiki_12.parquet\n",
      "wiki_13.parquet\n",
      "wiki_14.parquet\n",
      "wiki_15.parquet\n",
      "wiki_16.parquet\n",
      "wiki_17.parquet\n",
      "wiki_18.parquet\n",
      "wiki_19.parquet\n",
      "wiki_2.parquet\n",
      "wiki_20.parquet\n",
      "wiki_21.parquet\n",
      "wiki_22.parquet\n",
      "wiki_23.parquet\n",
      "wiki_24.parquet\n",
      "wiki_25.parquet\n",
      "wiki_26.parquet\n",
      "wiki_27.parquet\n",
      "wiki_28.parquet\n",
      "wiki_29.parquet\n",
      "wiki_3.parquet\n",
      "wiki_30.parquet\n",
      "wiki_31.parquet\n",
      "wiki_32.parquet\n",
      "wiki_33.parquet\n",
      "wiki_34.parquet\n",
      "wiki_35.parquet\n",
      "wiki_36.parquet\n",
      "wiki_37.parquet\n",
      "wiki_38.parquet\n",
      "wiki_39.parquet\n",
      "wiki_4.parquet\n",
      "wiki_40.parquet\n",
      "wiki_5.parquet\n",
      "wiki_6.parquet\n",
      "wiki_7.parquet\n",
      "wiki_8.parquet\n",
      "wiki_9.parquet\n",
      "82112\n"
     ]
    }
   ],
   "source": [
    "first = True\n",
    "for file in os.listdir('wiki'):\n",
    "    print(file)\n",
    "    if first:\n",
    "        df = filter_df(pd.read_parquet(f'wiki/{file}'), selected)\n",
    "        first = False\n",
    "        continue\n",
    "    df = pd.concat([df, filter_df(pd.read_parquet(f'wiki/{file}'), selected)], axis=0)\n",
    "df = df.drop_duplicates(['article_id'])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_blacklist = []\n",
    "with open('blacklist.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        title_blacklist.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31205\n"
     ]
    }
   ],
   "source": [
    "m = selected.merge(df[['article_id', 'lower_title']], on='lower_title', how='left')\n",
    "m = m[m.article_id.isna()]\n",
    "m = m[~m.lower_title.isin(title_blacklist)]\n",
    "m.to_parquet('data/selected_missing.parquet')\n",
    "print(len(m))\n",
    "del(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_id'] = df['article_id'].astype(int)\n",
    "df = df[~df.article_id.isin(ARTICLE_ID_BLACKLIST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f'data/articles_top{MIN_COUNT}_len{LENGTH}_stride{STRIDE}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_text(text):\n",
    "#     start = 0\n",
    "#     end = 75\n",
    "#     while end < len(text):\n",
    "#         print(text[start : end])\n",
    "#         start += 75\n",
    "#         end += 75\n",
    "#     print(text[end :])\n",
    "\n",
    "# print_text(chunks.sample().iloc[0]['chunk'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
