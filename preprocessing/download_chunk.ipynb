{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 5\n",
    "STRIDE = 15\n",
    "LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download EN wikipedia locally.\n",
    "\n",
    "# import requests\n",
    "# import io\n",
    "\n",
    "# WIKIPEDIA = [\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/0.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/1.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/2.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/3.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/4.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/5.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/6.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/7.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/8.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/9.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/10.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/11.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/12.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/13.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/14.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/15.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/16.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/17.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/18.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/19.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/20.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/21.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/22.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/23.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/24.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/25.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/26.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/27.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/28.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/29.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/30.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/31.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/32.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/33.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/34.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/35.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/36.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/37.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/38.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/39.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/40.parquet\"]\n",
    "\n",
    "# for i, url in enumerate(WIKIPEDIA):\n",
    "#     print(url)\n",
    "#     pd.read_parquet(io.BytesIO(requests.get(url).content)).rename(columns={'id': 'article_id'}).to_parquet(f\"wiki/wiki_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701412"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def select_top_counts(min_count):\n",
    "    selected = {}\n",
    "    with open('data/links.txt', 'r', encoding='utf-8', errors='replace') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Filter out hashtag pages\n",
    "                if line[0] == '#':\n",
    "                    continue\n",
    "\n",
    "                # Filter out category pages\n",
    "                skip = False\n",
    "                for substring in ['Category:', 'File:', 'Wikipedia:', 'user:']:\n",
    "                    if substring in line:\n",
    "                        skip = True\n",
    "                if skip:\n",
    "                    continue\n",
    "                    \n",
    "                # Filter out aliases and sublinks\n",
    "                for substring in ['#', '|']:\n",
    "                    if substring in line:\n",
    "                        line = line.split(substring, 1)[0]\n",
    "\n",
    "                line = line.strip()\n",
    "\n",
    "                if line in selected:\n",
    "                    selected[line] += 1\n",
    "                else:\n",
    "                    selected[line] = 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    selected = pd.DataFrame(\n",
    "        list(\n",
    "            filter(\n",
    "                lambda x: x['count'] >= min_count,\n",
    "                sorted(\n",
    "                    [\n",
    "                        {\n",
    "                            'lower_title': x.lower().strip(), \n",
    "                            'count': y\n",
    "                        } \n",
    "                        for x, y in selected.items()\n",
    "                    ],\n",
    "                    key=lambda x: x['count'],\n",
    "                    reverse=True\n",
    "                )   \n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return selected\n",
    "\n",
    "selected = select_top_counts(MIN_COUNT)\n",
    "selected.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Set \n",
    "\n",
    "def filter_df(df, selected):\n",
    "    df = df.rename(columns={'id': 'article_id'})\n",
    "    df['lower_title'] = df.title.apply(lambda x: x.strip().lower())\n",
    "    return df.merge(selected, on='lower_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_0.parquet\n",
      "wiki_1.parquet\n",
      "wiki_10.parquet\n",
      "wiki_11.parquet\n",
      "wiki_12.parquet\n",
      "wiki_13.parquet\n",
      "wiki_14.parquet\n",
      "wiki_15.parquet\n",
      "wiki_16.parquet\n",
      "wiki_17.parquet\n",
      "wiki_18.parquet\n",
      "wiki_19.parquet\n",
      "wiki_2.parquet\n",
      "wiki_20.parquet\n",
      "wiki_21.parquet\n",
      "wiki_22.parquet\n",
      "wiki_23.parquet\n",
      "wiki_24.parquet\n",
      "wiki_25.parquet\n",
      "wiki_26.parquet\n",
      "wiki_27.parquet\n",
      "wiki_28.parquet\n",
      "wiki_29.parquet\n",
      "wiki_3.parquet\n",
      "wiki_30.parquet\n",
      "wiki_31.parquet\n",
      "wiki_32.parquet\n",
      "wiki_33.parquet\n",
      "wiki_34.parquet\n",
      "wiki_35.parquet\n",
      "wiki_36.parquet\n",
      "wiki_37.parquet\n",
      "wiki_38.parquet\n",
      "wiki_39.parquet\n",
      "wiki_4.parquet\n",
      "wiki_40.parquet\n",
      "wiki_5.parquet\n",
      "wiki_6.parquet\n",
      "wiki_7.parquet\n",
      "wiki_8.parquet\n",
      "wiki_9.parquet\n",
      "501716\n"
     ]
    }
   ],
   "source": [
    "first = True\n",
    "for file in os.listdir('wiki'):\n",
    "    print(file)\n",
    "    if first:\n",
    "        df = filter_df(pd.read_parquet(f'wiki/{file}'), selected)\n",
    "        first = False\n",
    "        continue\n",
    "    df = pd.concat([df, filter_df(pd.read_parquet(f'wiki/{file}'), selected)], axis=0)\n",
    "df = df.drop_duplicates(['article_id'])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = selected.merge(df[['article_id', 'lower_title']], on='lower_title', how='left')\n",
    "m = m[m.article_id.isna()]\n",
    "m.to_parquet('data/selected_missing.parquet')\n",
    "del(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('text', axis=1).to_parquet(f'data/articles_top{MIN_COUNT}_len{LENGTH}_stride{STRIDE}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed article: 0\n",
      "processed article: 10000\n",
      "processed article: 20000\n",
      "processed article: 30000\n",
      "processed article: 40000\n",
      "processed article: 50000\n",
      "processed article: 60000\n",
      "processed article: 70000\n",
      "processed article: 80000\n",
      "processed article: 90000\n",
      "processed article: 100000\n",
      "processed article: 110000\n",
      "processed article: 120000\n",
      "processed article: 130000\n",
      "processed article: 140000\n",
      "processed article: 150000\n",
      "processed article: 160000\n",
      "processed article: 170000\n",
      "processed article: 180000\n",
      "processed article: 190000\n",
      "processed article: 200000\n",
      "processed article: 210000\n",
      "processed article: 220000\n",
      "processed article: 230000\n",
      "processed article: 240000\n",
      "processed article: 250000\n",
      "processed article: 260000\n",
      "processed article: 270000\n",
      "processed article: 280000\n",
      "processed article: 290000\n",
      "processed article: 300000\n",
      "processed article: 310000\n",
      "processed article: 320000\n",
      "processed article: 330000\n",
      "processed article: 340000\n",
      "processed article: 350000\n",
      "processed article: 360000\n",
      "processed article: 370000\n",
      "processed article: 380000\n",
      "processed article: 390000\n",
      "processed article: 400000\n",
      "processed article: 410000\n",
      "processed article: 420000\n",
      "processed article: 430000\n",
      "processed article: 440000\n",
      "processed article: 450000\n",
      "processed article: 460000\n",
      "processed article: 470000\n",
      "processed article: 480000\n",
      "processed article: 490000\n",
      "processed article: 500000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "buffer = [] \n",
    "BUFFER_SIZE = 100000\n",
    "chunk_id = 0\n",
    "articles_processed = 0\n",
    "\n",
    "def clear_buffer(buffer):\n",
    "    with open(f'data/chunks_top{MIN_COUNT}_len{LENGTH}_stride{STRIDE}.jsonl', 'a', encoding='utf-8') as file:\n",
    "        for row in buffer:\n",
    "            file.write(json.dumps(row) + '\\n')\n",
    "    return []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    if articles_processed % 10000 == 0:\n",
    "        print('processed article:', articles_processed)\n",
    "    start = 0\n",
    "    end = LENGTH\n",
    "    text = row['text'].split()\n",
    "    while end < len(text):\n",
    "        buffer.append({\n",
    "            'article_id': row['article_id'], \n",
    "            'chunk_id': chunk_id, \n",
    "            'chunk': ' '.join(text[start : end])\n",
    "        })\n",
    "        chunk_id += 1\n",
    "        \n",
    "        start += STRIDE \n",
    "        end += STRIDE\n",
    "\n",
    "        if len(buffer) >= BUFFER_SIZE:\n",
    "            buffer = clear_buffer(buffer)\n",
    "    \n",
    "    if start <= len(text):\n",
    "        buffer.append({\n",
    "            'article_id': row['article_id'], \n",
    "            'chunk_id': chunk_id, \n",
    "            'chunk': ' '.join(text[start :])\n",
    "        })\n",
    "        chunk_id += 1\n",
    "\n",
    "    articles_processed += 1\n",
    "\n",
    "if buffer:\n",
    "    buffer = clear_buffer(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_text(text):\n",
    "#     start = 0\n",
    "#     end = 75\n",
    "#     while end < len(text):\n",
    "#         print(text[start : end])\n",
    "#         start += 75\n",
    "#         end += 75\n",
    "#     print(text[end :])\n",
    "\n",
    "# print_text(chunks.sample().iloc[0]['chunk'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
