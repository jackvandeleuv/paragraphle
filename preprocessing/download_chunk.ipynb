{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download EN wikipedia locally.\n",
    "\n",
    "# import requests\n",
    "# import io\n",
    "\n",
    "# WIKIPEDIA = [\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/0.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/1.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/2.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/3.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/4.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/5.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/6.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/7.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/8.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/9.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/10.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/11.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/12.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/13.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/14.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/15.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/16.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/17.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/18.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/19.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/20.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/21.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/22.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/23.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/24.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/25.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/26.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/27.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/28.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/29.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/30.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/31.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/32.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/33.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/34.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/35.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/36.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/37.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/38.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/39.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/40.parquet\"]\n",
    "\n",
    "# for i, url in enumerate(WIKIPEDIA):\n",
    "#     print(url)\n",
    "#     pd.read_parquet(io.BytesIO(requests.get(url).content)).rename(columns={'id': 'article_id'}).to_parquet(f\"wiki/wiki_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79406"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def select_top_counts():\n",
    "    selected = {}\n",
    "    with open('../transformed/data/links.txt', 'r', encoding='utf-8', errors='replace') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Filter out hashtag pages\n",
    "                if line[0] == '#':\n",
    "                    continue\n",
    "\n",
    "                # Filter out category pages\n",
    "                skip = False\n",
    "                for substring in ['Category:', 'File:', 'Wikipedia:', 'user:']:\n",
    "                    if substring in line:\n",
    "                        skip = True\n",
    "                if skip:\n",
    "                    continue\n",
    "                    \n",
    "                # Filter out aliases and sublinks\n",
    "                for substring in ['#', '|']:\n",
    "                    if substring in line:\n",
    "                        line = line.split(substring, 1)[0]\n",
    "\n",
    "                line = line.strip()\n",
    "\n",
    "                if line in selected:\n",
    "                    selected[line] += 1\n",
    "                else:\n",
    "                    selected[line] = 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    selected = pd.DataFrame(\n",
    "        list(\n",
    "            filter(\n",
    "                lambda x: x['count'] >= 35,\n",
    "                sorted(\n",
    "                    [\n",
    "                        {\n",
    "                            'lower_title': x.lower().strip(), \n",
    "                            'count': y\n",
    "                        } \n",
    "                        for x, y in selected.items()\n",
    "                    ],\n",
    "                    key=lambda x: x['count'],\n",
    "                    reverse=True\n",
    "                )   \n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return selected\n",
    "\n",
    "selected = select_top_counts()\n",
    "selected.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Set \n",
    "\n",
    "def filter_df(df, selected):\n",
    "    df = df.rename(columns={'id': 'article_id'})\n",
    "    df['lower_title'] = df.title.apply(lambda x: x.strip().lower())\n",
    "    return df.merge(selected, on='lower_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_0.parquet\n",
      "wiki_1.parquet\n",
      "wiki_10.parquet\n",
      "wiki_11.parquet\n",
      "wiki_12.parquet\n",
      "wiki_13.parquet\n",
      "wiki_14.parquet\n",
      "wiki_15.parquet\n",
      "wiki_16.parquet\n",
      "wiki_17.parquet\n",
      "wiki_18.parquet\n",
      "wiki_19.parquet\n",
      "wiki_2.parquet\n",
      "wiki_20.parquet\n",
      "wiki_21.parquet\n",
      "wiki_22.parquet\n",
      "wiki_23.parquet\n",
      "wiki_24.parquet\n",
      "wiki_25.parquet\n",
      "wiki_26.parquet\n",
      "wiki_27.parquet\n",
      "wiki_28.parquet\n",
      "wiki_29.parquet\n",
      "wiki_3.parquet\n",
      "wiki_30.parquet\n",
      "wiki_31.parquet\n",
      "wiki_32.parquet\n",
      "wiki_33.parquet\n",
      "wiki_34.parquet\n",
      "wiki_35.parquet\n",
      "wiki_36.parquet\n",
      "wiki_37.parquet\n",
      "wiki_38.parquet\n",
      "wiki_39.parquet\n",
      "wiki_4.parquet\n",
      "wiki_40.parquet\n",
      "wiki_5.parquet\n",
      "wiki_6.parquet\n",
      "wiki_7.parquet\n",
      "wiki_8.parquet\n",
      "wiki_9.parquet\n"
     ]
    }
   ],
   "source": [
    "first = True\n",
    "for file in os.listdir('wiki'):\n",
    "    print(file)\n",
    "    if first:\n",
    "        df = filter_df(pd.read_parquet(f'wiki/{file}'), selected)\n",
    "        first = False\n",
    "        continue\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df, \n",
    "            filter_df(pd.read_parquet(f'wiki/{file}'), selected)\n",
    "        ], \n",
    "        axis=0\n",
    "    )\n",
    "df = df.drop_duplicates(['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('text', axis=1).to_parquet('data/articles_over_35.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7671920"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = [] \n",
    "i = 0\n",
    "stride = 25\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    start = 0\n",
    "    end = 50\n",
    "    text = row['text'].split()\n",
    "    while end < len(text):\n",
    "        chunks.append({\n",
    "            'article_id': row['article_id'], \n",
    "            'chunk_id': i, \n",
    "            'chunk': ' '.join(text[start : end])\n",
    "        })\n",
    "        i += 1\n",
    "        \n",
    "        start += stride \n",
    "        end += stride\n",
    "    \n",
    "    if start <= len(text):\n",
    "        chunks.append({\n",
    "            'article_id': row['article_id'], \n",
    "            'chunk_id': i, \n",
    "            'chunk': ' '.join(text[start :])\n",
    "        })\n",
    "        i += 1\n",
    "\n",
    "del(df)\n",
    "chunks = pd.DataFrame(chunks)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.to_parquet('data/chunks_over_35.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included Chattanooga, Tennessee natives Sam Gooden, Richard Brooks, and his brother Arthur Brooks. B\n",
      "y 1958, the Roosters had a new manager in Eddie Thomas, a record deal with Vee-Jay Records, and a ne\n",
      "w name: Jerry Butler & the Impressions. The group's first hit single was 1958's \"For Your Precious L\n",
      "ove\", which hit No. 11 on the US pop chart and No. 3 on the R&B chart. However, soon after the relea\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_text(text):\n",
    "    start = 0\n",
    "    end = 100\n",
    "    while end < len(text):\n",
    "        print(text[start : end])\n",
    "        start += 100\n",
    "        end += 100\n",
    "    print(text[end :])\n",
    "\n",
    "print_text(chunks.sample().iloc[0]['chunk'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
