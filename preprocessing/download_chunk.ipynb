{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 5\n",
    "STRIDE = 15\n",
    "LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mwparserfromhell\n",
      "  Downloading mwparserfromhell-0.7.2-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Downloading mwparserfromhell-0.7.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "   ---------------------------------------- 0.0/156.4 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 61.4/156.4 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 156.4/156.4 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: mwparserfromhell\n",
      "Successfully installed mwparserfromhell-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "https://dumps.wikimedia.org/enwiki/20220920/dumpstatus.json",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\fsspec\\implementations\\http.py:422\u001b[0m, in \u001b[0;36mHTTPFileSystem._info\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     info\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m--> 422\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m _file_info(\n\u001b[0;32m    423\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_url(url),\n\u001b[0;32m    424\u001b[0m             size_policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[0;32m    425\u001b[0m             session\u001b[38;5;241m=\u001b[39msession,\n\u001b[0;32m    426\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    427\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    428\u001b[0m         )\n\u001b[0;32m    429\u001b[0m     )\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\fsspec\\implementations\\http.py:835\u001b[0m, in \u001b[0;36m_file_info\u001b[1;34m(url, session, size_policy, **kwargs)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m r:\n\u001b[1;32m--> 835\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;66;03m#  recognise lack of 'Accept-Ranges',\u001b[39;00m\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;66;03m#                 or 'Accept-Ranges': 'none' (not 'bytes')\u001b[39;00m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;66;03m#  to mean streaming only, no random access => return None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\aiohttp\\client_reqrep.py:1161\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m-> 1161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[0;32m   1164\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m   1165\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[0;32m   1166\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1167\u001b[0m )\n",
      "\u001b[1;31mClientResponseError\u001b[0m: 404, message='Not Found', url='https://dumps.wikimedia.org/enwiki/20220920/dumpstatus.json'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> 3\u001b[0m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molm/wikipedia\u001b[39m\u001b[38;5;124m\"\u001b[39m, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20220920\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\load.py:2154\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m   2153\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2154\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[0;32m   2155\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2156\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2157\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m   2158\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[0;32m   2159\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2160\u001b[0m )\n\u001b[0;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2165\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m--> 924\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    925\u001b[0m     dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m    926\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    929\u001b[0m )\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:1648\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[1;32m-> 1648\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1649\u001b[0m         dl_manager,\n\u001b[0;32m   1650\u001b[0m         verification_mode,\n\u001b[0;32m   1651\u001b[0m         check_duplicate_keys\u001b[38;5;241m=\u001b[39mverification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS\n\u001b[0;32m   1652\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS,\n\u001b[0;32m   1653\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs,\n\u001b[0;32m   1654\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:978\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[0;32m    977\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[1;32m--> 978\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_generators_kwargs)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\olm--wikipedia\\01bdd02b1dcc8095535ad5539fbecf3a3a9bfe07c94227b8e93ea57e0722aec0\\wikipedia.py:949\u001b[0m, in \u001b[0;36mWikipedia._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    947\u001b[0m info_url \u001b[38;5;241m=\u001b[39m _base_url(lang) \u001b[38;5;241m+\u001b[39m _INFO_FILE\n\u001b[0;32m    948\u001b[0m \u001b[38;5;66;03m# Use dictionary since testing mock always returns the same result.\u001b[39;00m\n\u001b[1;32m--> 949\u001b[0m downloaded_files \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownload_and_extract({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m\"\u001b[39m: info_url})\n\u001b[0;32m    951\u001b[0m xml_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    952\u001b[0m total_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\download\\download_manager.py:326\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_urls):\n\u001b[0;32m    311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    Is roughly equivalent to:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload(url_or_urls))\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\download\\download_manager.py:159\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    157\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[1;32m--> 159\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m map_nested(\n\u001b[0;32m    160\u001b[0m         download_func,\n\u001b[0;32m    161\u001b[0m         url_or_urls,\n\u001b[0;32m    162\u001b[0m         map_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    163\u001b[0m         num_proc\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mnum_proc,\n\u001b[0;32m    164\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading data files\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    165\u001b[0m         batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    166\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\utils\\py_utils.py:512\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[0;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[0;32m    511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 512\u001b[0m     _single_map_nested((function, obj, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[0;32m    514\u001b[0m ]\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[0;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\utils\\py_utils.py:380\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    375\u001b[0m     batched\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[0;32m    379\u001b[0m ):\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\download\\download_manager.py:220\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[1;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[0;32m    207\u001b[0m         download_func,\n\u001b[0;32m    208\u001b[0m         url_or_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[0;32m    222\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\download\\download_manager.py:229\u001b[0m, in \u001b[0;36mDownloadManager._download_single\u001b[1;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m url_or_path_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path, url_or_filename)\n\u001b[1;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m cached_path(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[0;32m    230\u001b[0m out \u001b[38;5;241m=\u001b[39m tracked_str(out)\n\u001b[0;32m    231\u001b[0m out\u001b[38;5;241m.\u001b[39mset_origin(url_or_filename)\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\utils\\file_utils.py:205\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# Download external files\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         output_path \u001b[38;5;241m=\u001b[39m get_from_cache(\n\u001b[0;32m    206\u001b[0m             url_or_filename,\n\u001b[0;32m    207\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    208\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mforce_download,\n\u001b[0;32m    209\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muser_agent,\n\u001b[0;32m    210\u001b[0m             use_etag\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_etag,\n\u001b[0;32m    211\u001b[0m             token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m    212\u001b[0m             storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    213\u001b[0m             download_desc\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mdownload_desc,\n\u001b[0;32m    214\u001b[0m             disable_tqdm\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mdisable_tqdm,\n\u001b[0;32m    215\u001b[0m         )\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\utils\\file_utils.py:382\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, user_agent, use_etag, token, storage_options, download_desc, disable_tqdm)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_agent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m user_agent\n\u001b[1;32m--> 382\u001b[0m response \u001b[38;5;241m=\u001b[39m fsspec_head(url, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m    383\u001b[0m etag \u001b[38;5;241m=\u001b[39m (response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mif\u001b[39;00m use_etag \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# Try a second time\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\datasets\\utils\\file_utils.py:296\u001b[0m, in \u001b[0;36mfsspec_head\u001b[1;34m(url, storage_options)\u001b[0m\n\u001b[0;32m    294\u001b[0m _raise_if_offline_mode_is_enabled(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to reach \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m fs, path \u001b[38;5;241m=\u001b[39m url_to_fs(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fs\u001b[38;5;241m.\u001b[39minfo(path)\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\fsspec\\asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\fsspec\\asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\fsspec\\asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[1;34m(event, coro, result, timeout)\u001b[0m\n\u001b[0;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[1;32mc:\\Users\\jackv\\anaconda3\\Lib\\site-packages\\fsspec\\implementations\\http.py:435\u001b[0m, in \u001b[0;36mHTTPFileSystem._info\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m policy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    434\u001b[0m             \u001b[38;5;66;03m# If get failed, then raise a FileNotFoundError\u001b[39;00m\n\u001b[1;32m--> 435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(url) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    436\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39mexc)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minfo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: https://dumps.wikimedia.org/enwiki/20220920/dumpstatus.json"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "load_dataset(\"olm/wikipedia\", language=\"en\", date=\"20220920\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download EN wikipedia locally.\n",
    "\n",
    "# import requests\n",
    "# import io\n",
    "\n",
    "# WIKIPEDIA = [\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/0.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/1.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/2.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/3.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/4.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/5.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/6.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/7.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/8.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/9.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/10.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/11.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/12.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/13.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/14.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/15.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/16.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/17.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/18.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/19.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/20.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/21.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/22.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/23.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/24.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/25.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/26.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/27.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/28.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/29.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/30.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/31.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/32.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/33.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/34.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/35.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/36.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/37.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/38.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/39.parquet\",\"https://huggingface.co/api/datasets/wikimedia/wikipedia/parquet/20231101.en/train/40.parquet\"]\n",
    "\n",
    "# for i, url in enumerate(WIKIPEDIA):\n",
    "#     print(url)\n",
    "#     pd.read_parquet(io.BytesIO(requests.get(url).content)).rename(columns={'id': 'article_id'}).to_parquet(f\"wiki/wiki_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701412"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def select_top_counts(min_count):\n",
    "    selected = {}\n",
    "    with open('data/links.txt', 'r', encoding='utf-8', errors='replace') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Filter out hashtag pages\n",
    "                if line[0] == '#':\n",
    "                    continue\n",
    "\n",
    "                # Filter out category pages\n",
    "                skip = False\n",
    "                for substring in ['Category:', 'File:', 'Wikipedia:', 'user:']:\n",
    "                    if substring in line:\n",
    "                        skip = True\n",
    "                if skip:\n",
    "                    continue\n",
    "                    \n",
    "                # Filter out aliases and sublinks\n",
    "                for substring in ['#', '|']:\n",
    "                    if substring in line:\n",
    "                        line = line.split(substring, 1)[0]\n",
    "\n",
    "                line = line.strip()\n",
    "\n",
    "                if line in selected:\n",
    "                    selected[line] += 1\n",
    "                else:\n",
    "                    selected[line] = 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    selected = pd.DataFrame(\n",
    "        list(\n",
    "            filter(\n",
    "                lambda x: x['count'] >= min_count,\n",
    "                sorted(\n",
    "                    [\n",
    "                        {\n",
    "                            'lower_title': x.lower().strip(), \n",
    "                            'count': y\n",
    "                        } \n",
    "                        for x, y in selected.items()\n",
    "                    ],\n",
    "                    key=lambda x: x['count'],\n",
    "                    reverse=True\n",
    "                )   \n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return selected\n",
    "\n",
    "selected = select_top_counts(MIN_COUNT)\n",
    "selected.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Set \n",
    "\n",
    "def filter_df(df, selected):\n",
    "    df = df.rename(columns={'id': 'article_id'})\n",
    "    df['lower_title'] = df.title.apply(lambda x: x.strip().lower())\n",
    "    return df.merge(selected, on='lower_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_0.parquet\n",
      "wiki_1.parquet\n",
      "wiki_10.parquet\n",
      "wiki_11.parquet\n",
      "wiki_12.parquet\n",
      "wiki_13.parquet\n",
      "wiki_14.parquet\n",
      "wiki_15.parquet\n",
      "wiki_16.parquet\n",
      "wiki_17.parquet\n",
      "wiki_18.parquet\n",
      "wiki_19.parquet\n",
      "wiki_2.parquet\n",
      "wiki_20.parquet\n",
      "wiki_21.parquet\n",
      "wiki_22.parquet\n",
      "wiki_23.parquet\n",
      "wiki_24.parquet\n",
      "wiki_25.parquet\n",
      "wiki_26.parquet\n",
      "wiki_27.parquet\n",
      "wiki_28.parquet\n",
      "wiki_29.parquet\n",
      "wiki_3.parquet\n",
      "wiki_30.parquet\n",
      "wiki_31.parquet\n",
      "wiki_32.parquet\n",
      "wiki_33.parquet\n",
      "wiki_34.parquet\n",
      "wiki_35.parquet\n",
      "wiki_36.parquet\n",
      "wiki_37.parquet\n",
      "wiki_38.parquet\n",
      "wiki_39.parquet\n",
      "wiki_4.parquet\n",
      "wiki_40.parquet\n",
      "wiki_5.parquet\n",
      "wiki_6.parquet\n",
      "wiki_7.parquet\n",
      "wiki_8.parquet\n",
      "wiki_9.parquet\n",
      "501716\n"
     ]
    }
   ],
   "source": [
    "first = True\n",
    "for file in os.listdir('wiki'):\n",
    "    print(file)\n",
    "    if first:\n",
    "        df = filter_df(pd.read_parquet(f'wiki/{file}'), selected)\n",
    "        first = False\n",
    "        continue\n",
    "    df = pd.concat([df, filter_df(pd.read_parquet(f'wiki/{file}'), selected)], axis=0)\n",
    "df = df.drop_duplicates(['article_id'])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = selected.merge(df[['article_id', 'lower_title']], on='lower_title', how='left')\n",
    "m = m[m.article_id.isna()]\n",
    "m.to_parquet('data/selected_missing.parquet')\n",
    "del(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('text', axis=1).to_parquet(f'data/articles_top{MIN_COUNT}_len{LENGTH}_stride{STRIDE}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed article: 0\n",
      "processed article: 10000\n",
      "processed article: 20000\n",
      "processed article: 30000\n",
      "processed article: 40000\n",
      "processed article: 50000\n",
      "processed article: 60000\n",
      "processed article: 70000\n",
      "processed article: 80000\n",
      "processed article: 90000\n",
      "processed article: 100000\n",
      "processed article: 110000\n",
      "processed article: 120000\n",
      "processed article: 130000\n",
      "processed article: 140000\n",
      "processed article: 150000\n",
      "processed article: 160000\n",
      "processed article: 170000\n",
      "processed article: 180000\n",
      "processed article: 190000\n",
      "processed article: 200000\n",
      "processed article: 210000\n",
      "processed article: 220000\n",
      "processed article: 230000\n",
      "processed article: 240000\n",
      "processed article: 250000\n",
      "processed article: 260000\n",
      "processed article: 270000\n",
      "processed article: 280000\n",
      "processed article: 290000\n",
      "processed article: 300000\n",
      "processed article: 310000\n",
      "processed article: 320000\n",
      "processed article: 330000\n",
      "processed article: 340000\n",
      "processed article: 350000\n",
      "processed article: 360000\n",
      "processed article: 370000\n",
      "processed article: 380000\n",
      "processed article: 390000\n",
      "processed article: 400000\n",
      "processed article: 410000\n",
      "processed article: 420000\n",
      "processed article: 430000\n",
      "processed article: 440000\n",
      "processed article: 450000\n",
      "processed article: 460000\n",
      "processed article: 470000\n",
      "processed article: 480000\n",
      "processed article: 490000\n",
      "processed article: 500000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "buffer = [] \n",
    "BUFFER_SIZE = 100000\n",
    "chunk_id = 0\n",
    "articles_processed = 0\n",
    "\n",
    "def clear_buffer(buffer):\n",
    "    with open(f'data/chunks_top{MIN_COUNT}_len{LENGTH}_stride{STRIDE}.jsonl', 'a', encoding='utf-8') as file:\n",
    "        for row in buffer:\n",
    "            file.write(json.dumps(row) + '\\n')\n",
    "    return []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    if articles_processed % 10000 == 0:\n",
    "        print('processed article:', articles_processed)\n",
    "    start = 0\n",
    "    end = LENGTH\n",
    "    text = row['text'].split()\n",
    "    while end < len(text):\n",
    "        buffer.append({\n",
    "            'article_id': row['article_id'], \n",
    "            'chunk_id': chunk_id, \n",
    "            'chunk': ' '.join(text[start : end])\n",
    "        })\n",
    "        chunk_id += 1\n",
    "        \n",
    "        start += STRIDE \n",
    "        end += STRIDE\n",
    "\n",
    "        if len(buffer) >= BUFFER_SIZE:\n",
    "            buffer = clear_buffer(buffer)\n",
    "    \n",
    "    if start <= len(text):\n",
    "        buffer.append({\n",
    "            'article_id': row['article_id'], \n",
    "            'chunk_id': chunk_id, \n",
    "            'chunk': ' '.join(text[start :])\n",
    "        })\n",
    "        chunk_id += 1\n",
    "\n",
    "    articles_processed += 1\n",
    "\n",
    "if buffer:\n",
    "    buffer = clear_buffer(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_text(text):\n",
    "#     start = 0\n",
    "#     end = 75\n",
    "#     while end < len(text):\n",
    "#         print(text[start : end])\n",
    "#         start += 75\n",
    "#         end += 75\n",
    "#     print(text[end :])\n",
    "\n",
    "# print_text(chunks.sample().iloc[0]['chunk'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
