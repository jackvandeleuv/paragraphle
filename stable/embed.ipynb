{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os \n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "def embed_chunk(df, i, client, window):\n",
    "    window = df.iloc[i : i + window]\n",
    "    ids = window['id'].values.tolist()\n",
    "\n",
    "    try:\n",
    "        result = client.embeddings.create(\n",
    "            input=window['clean_text'].values.tolist(),\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return [\n",
    "            {'id': x, 'embedding': None} \n",
    "            for x in ids\n",
    "        ]\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            'id': ids[i], \n",
    "            'embedding': base64.b64encode(\n",
    "                np.array(\n",
    "                    x.embedding\n",
    "                ).tobytes()\n",
    "            ).decode('utf-8'), \n",
    "            'model': \"text-embedding-3-small\"\n",
    "        } \n",
    "        for i, x in enumerate(result.data)\n",
    "    ]\n",
    "    \n",
    "\n",
    "def embed_df(df, client):\n",
    "    WINDOW = 1000\n",
    "\n",
    "    embeddings = []\n",
    "    i = 0\n",
    "    while i + WINDOW < len(df):\n",
    "        embeddings.extend(embed_chunk(df, i, client, WINDOW))\n",
    "        i += WINDOW\n",
    "\n",
    "    if i < len(df):\n",
    "        embeddings.extend(embed_chunk(df, i, client, WINDOW))\n",
    "\n",
    "    del(df)\n",
    "\n",
    "    with open('data/embeddings.jsonl', 'a', encoding='utf-8') as file:\n",
    "        for row in embeddings:\n",
    "            file.write(json.dumps(row) + '\\n')\n",
    "\n",
    "    del(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed buffer #1\n",
      "Processed buffer #2\n",
      "Processed buffer #3\n",
      "Processed buffer #4\n",
      "Processed buffer #5\n",
      "Processed buffer #6\n",
      "Processed buffer #7\n",
      "Processed buffer #8\n",
      "Processed buffer #9\n",
      "Processed buffer #10\n",
      "Processed buffer #11\n",
      "Processed buffer #12\n",
      "Processed buffer #13\n",
      "Processed buffer #14\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 64390 tokens (64390 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processed buffer #15\n",
      "Processed buffer #16\n",
      "Processed buffer #17\n",
      "Processed buffer #18\n",
      "Processed buffer #19\n",
      "Processed buffer #20\n",
      "Processed buffer #21\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 18014 tokens (18014 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processed buffer #22\n",
      "Processed buffer #23\n",
      "Processed buffer #24\n",
      "Processed buffer #25\n",
      "Processed buffer #26\n",
      "Processed buffer #27\n",
      "Processed buffer #28\n",
      "Processed buffer #29\n",
      "Processed buffer #30\n",
      "Processed buffer #31\n",
      "Processed buffer #32\n",
      "Processed buffer #33\n",
      "Processed buffer #34\n",
      "Processed buffer #35\n",
      "Processed buffer #36\n",
      "Processed buffer #37\n",
      "Processed buffer #38\n",
      "Processed buffer #39\n",
      "Processed buffer #40\n",
      "Processed buffer #41\n",
      "Processed buffer #42\n",
      "Processed buffer #43\n",
      "Processed buffer #44\n",
      "Processed buffer #45\n",
      "Processed buffer #46\n",
      "Processed buffer #47\n",
      "Processed buffer #48\n",
      "Processed buffer #49\n",
      "Processed buffer #50\n",
      "Processed buffer #51\n",
      "Processed buffer #52\n",
      "Processed buffer #53\n",
      "Processed buffer #54\n",
      "Processed buffer #55\n",
      "Processed buffer #56\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"API_KEY\"))\n",
    "\n",
    "i = 0\n",
    "\n",
    "with open('data/cleaned.jsonl', 'r', encoding='utf-8') as file:\n",
    "    buffer = []\n",
    "    for line in file:\n",
    "        buffer.append(json.loads(line))\n",
    "        if len(buffer) >= BUFFER_SIZE:\n",
    "            buffer = pd.DataFrame(buffer)\n",
    "            i += 1\n",
    "            print(f\"Processed buffer #{i}\")\n",
    "            embed_df(buffer, client)\n",
    "            buffer = []\n",
    "\n",
    "if len(buffer) >= 0:\n",
    "    buffer = pd.DataFrame(buffer)\n",
    "    i += 1\n",
    "    print(f\"Processed buffer #{i}\")\n",
    "    embed_df(buffer, client)\n",
    "    buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode:\n",
    "# np.frombuffer(base64.b64decode(j['embedding']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
